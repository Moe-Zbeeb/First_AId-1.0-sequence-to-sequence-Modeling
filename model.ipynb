{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mohammad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random \n",
    "import tensorflow    as tf\n",
    "import json \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents (2).json\") as file:\n",
    "    data = json.load(file)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define preprocessing functions\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "# Initialize lists for words, labels, training data, and output\n",
    "words = []\n",
    "labels = []\n",
    "training_data = []\n",
    "output_data = []\n",
    "\n",
    "# Tokenize and stem each pattern, and prepare training data and output\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        training_data.append(tokens)\n",
    "        output_data.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "# Stem and lowercase words, remove duplicates, and sort\n",
    "words = sorted(list(set([stem(word) for word in words if word != \"?\"])))\n",
    "labels = sorted(labels)\n",
    "\n",
    "# Initialize empty output rows\n",
    "out_empty = [0] * len(labels)\n",
    "\n",
    "# Prepare training and output arrays\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for idx, doc in enumerate(training_data):\n",
    "    bag = []\n",
    "\n",
    "    # Stem and lowercase words in the pattern\n",
    "    tokens = [stem(word) for word in doc]\n",
    "\n",
    "    # Create bag of words\n",
    "    for word in words:\n",
    "        bag.append(1) if word in tokens else bag.append(0)\n",
    "\n",
    "    # Create output row\n",
    "    output_row = list(out_empty)\n",
    "    output_row[labels.index(output_data[idx])] = 1\n",
    "\n",
    "    X_train.append(bag)\n",
    "    y_train.append(output_row)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 301)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 2416      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 99)                891       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3379 (13.20 KB)\n",
      "Trainable params: 3379 (13.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 0 completed out of 100 loss: 363.3642077445984\n",
      "Epoch 1 completed out of 100 loss: 360.441858291626\n",
      "Epoch 2 completed out of 100 loss: 355.9505805969238\n",
      "Epoch 3 completed out of 100 loss: 345.5143368244171\n",
      "Epoch 4 completed out of 100 loss: 328.9581274986267\n",
      "Epoch 5 completed out of 100 loss: 312.409068107605\n",
      "Epoch 6 completed out of 100 loss: 297.93339014053345\n",
      "Epoch 7 completed out of 100 loss: 284.8949600458145\n",
      "Epoch 8 completed out of 100 loss: 273.1415100097656\n",
      "Epoch 9 completed out of 100 loss: 262.7824849486351\n",
      "Epoch 10 completed out of 100 loss: 253.49018573760986\n",
      "Epoch 11 completed out of 100 loss: 244.85175210237503\n",
      "Epoch 12 completed out of 100 loss: 236.42868521809578\n",
      "Epoch 13 completed out of 100 loss: 228.31235525012016\n",
      "Epoch 14 completed out of 100 loss: 220.16919419169426\n",
      "Epoch 15 completed out of 100 loss: 212.25205597281456\n",
      "Epoch 16 completed out of 100 loss: 204.4545421898365\n",
      "Epoch 17 completed out of 100 loss: 196.78328415751457\n",
      "Epoch 18 completed out of 100 loss: 189.1314625442028\n",
      "Epoch 19 completed out of 100 loss: 181.92894832789898\n",
      "Epoch 20 completed out of 100 loss: 174.7563191652298\n",
      "Epoch 21 completed out of 100 loss: 167.87192291021347\n",
      "Epoch 22 completed out of 100 loss: 161.29401849210262\n",
      "Epoch 23 completed out of 100 loss: 155.01924628019333\n",
      "Epoch 24 completed out of 100 loss: 148.9926804304123\n",
      "Epoch 25 completed out of 100 loss: 143.22948091477156\n",
      "Epoch 26 completed out of 100 loss: 137.70783660560846\n",
      "Epoch 27 completed out of 100 loss: 132.4193615615368\n",
      "Epoch 28 completed out of 100 loss: 127.53754134476185\n",
      "Epoch 29 completed out of 100 loss: 122.68693515658379\n",
      "Epoch 30 completed out of 100 loss: 118.10991111397743\n",
      "Epoch 31 completed out of 100 loss: 113.83977987617254\n",
      "Epoch 32 completed out of 100 loss: 109.66248521953821\n",
      "Epoch 33 completed out of 100 loss: 105.62890576198697\n",
      "Epoch 34 completed out of 100 loss: 101.76875813677907\n",
      "Epoch 35 completed out of 100 loss: 98.07776010781527\n",
      "Epoch 36 completed out of 100 loss: 94.40700602531433\n",
      "Epoch 37 completed out of 100 loss: 90.97026707231998\n",
      "Epoch 38 completed out of 100 loss: 87.71864376962185\n",
      "Epoch 39 completed out of 100 loss: 84.51569846644998\n",
      "Epoch 40 completed out of 100 loss: 81.44396107271314\n",
      "Epoch 41 completed out of 100 loss: 78.50705626793206\n",
      "Epoch 42 completed out of 100 loss: 75.539081774652\n",
      "Epoch 43 completed out of 100 loss: 72.73630070313811\n",
      "Epoch 44 completed out of 100 loss: 70.00599595718086\n",
      "Epoch 45 completed out of 100 loss: 67.42578212916851\n",
      "Epoch 46 completed out of 100 loss: 64.96141223050654\n",
      "Epoch 47 completed out of 100 loss: 62.43268276937306\n",
      "Epoch 48 completed out of 100 loss: 60.14459147956222\n",
      "Epoch 49 completed out of 100 loss: 57.90402597095817\n",
      "Epoch 50 completed out of 100 loss: 55.66288800165057\n",
      "Epoch 51 completed out of 100 loss: 53.54030442982912\n",
      "Epoch 52 completed out of 100 loss: 51.567931353114545\n",
      "Epoch 53 completed out of 100 loss: 49.58939287811518\n",
      "Epoch 54 completed out of 100 loss: 47.69620583578944\n",
      "Epoch 55 completed out of 100 loss: 45.89436111692339\n",
      "Epoch 56 completed out of 100 loss: 44.13902676478028\n",
      "Epoch 57 completed out of 100 loss: 42.412520337849855\n",
      "Epoch 58 completed out of 100 loss: 40.764130832627416\n",
      "Epoch 59 completed out of 100 loss: 39.21853998536244\n",
      "Epoch 60 completed out of 100 loss: 37.670577006880194\n",
      "Epoch 61 completed out of 100 loss: 36.15036388486624\n",
      "Epoch 62 completed out of 100 loss: 34.6993323401548\n",
      "Epoch 63 completed out of 100 loss: 33.370515580289066\n",
      "Epoch 64 completed out of 100 loss: 31.985683511942625\n",
      "Epoch 65 completed out of 100 loss: 30.6380540500395\n",
      "Epoch 66 completed out of 100 loss: 29.41205266583711\n",
      "Epoch 67 completed out of 100 loss: 28.218591379001737\n",
      "Epoch 68 completed out of 100 loss: 27.04818511987105\n",
      "Epoch 69 completed out of 100 loss: 25.95467229001224\n",
      "Epoch 70 completed out of 100 loss: 24.885045995702967\n",
      "Epoch 71 completed out of 100 loss: 23.80861081602052\n",
      "Epoch 72 completed out of 100 loss: 22.852954187896103\n",
      "Epoch 73 completed out of 100 loss: 21.84728915989399\n",
      "Epoch 74 completed out of 100 loss: 20.921644844813272\n",
      "Epoch 75 completed out of 100 loss: 20.037540558027104\n",
      "Epoch 76 completed out of 100 loss: 19.20463205850683\n",
      "Epoch 77 completed out of 100 loss: 18.378020559903234\n",
      "Epoch 78 completed out of 100 loss: 17.579675978515297\n",
      "Epoch 79 completed out of 100 loss: 16.843685729894787\n",
      "Epoch 80 completed out of 100 loss: 16.09804234141484\n",
      "Epoch 81 completed out of 100 loss: 15.401967341662385\n",
      "Epoch 82 completed out of 100 loss: 14.757111276965588\n",
      "Epoch 83 completed out of 100 loss: 14.106623140862212\n",
      "Epoch 84 completed out of 100 loss: 13.477041664300486\n",
      "Epoch 85 completed out of 100 loss: 12.897930168779567\n",
      "Epoch 86 completed out of 100 loss: 12.326367191155441\n",
      "Epoch 87 completed out of 100 loss: 11.7814979046816\n",
      "Epoch 88 completed out of 100 loss: 11.253142450121231\n",
      "Epoch 89 completed out of 100 loss: 10.742683463846333\n",
      "Epoch 90 completed out of 100 loss: 10.284977748058736\n",
      "Epoch 91 completed out of 100 loss: 9.825005519320257\n",
      "Epoch 92 completed out of 100 loss: 9.38531406049151\n",
      "Epoch 93 completed out of 100 loss: 8.954354054410942\n",
      "Epoch 94 completed out of 100 loss: 8.565920910506975\n",
      "Epoch 95 completed out of 100 loss: 8.188016768777743\n",
      "Epoch 96 completed out of 100 loss: 7.80979258462321\n",
      "Epoch 97 completed out of 100 loss: 7.465455176366959\n",
      "Epoch 98 completed out of 100 loss: 7.133596939849667\n",
      "Epoch 99 completed out of 100 loss: 6.807812805578578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/.local/lib/python3.8/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "hidden1_size = 8\n",
    "hidden2_size = 8\n",
    "\n",
    "# Define the input and output shapes\n",
    "input_shape = len(words)\n",
    "output_shape = len(labels)\n",
    "\n",
    "# Define the input layer\n",
    "input_data = tf.keras.Input(shape=(input_shape,))\n",
    "\n",
    "# Define the hidden layers\n",
    "hidden1 = tf.keras.layers.Dense(hidden1_size, activation='relu')(input_data)\n",
    "hidden2 = tf.keras.layers.Dense(hidden2_size, activation='relu')(hidden1)\n",
    "\n",
    "# Define the output layer\n",
    "output = tf.keras.layers.Dense(output_shape, activation='softmax')(hidden2)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        batch_loss = model.train_on_batch(batch_X, batch_y)\n",
    "        epoch_loss += batch_loss[0]\n",
    "    print('Epoch', epoch, 'completed out of', num_epochs, 'loss:', epoch_loss)\n",
    "\n",
    "# Once trained, you can use the model for prediction\n",
    "predictions = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s,words):\n",
    "\tbag = [0 for _ in range(len(words))]\n",
    "\n",
    "\n",
    "\ts_words = nltk.word_tokenize(s)\n",
    "\ts_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "\tfor se in s_words:\n",
    "\t\tfor i, w in enumerate(words):\n",
    "\t\t\tif w == se:\n",
    "\t\t\t\tbag[i] = 1\n",
    "\n",
    "\treturn numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Start Talking with the bot(type quit to stop!)\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        # Convert user input to bag of words\n",
    "        bow = bag_of_words(inp, words)\n",
    "\n",
    "        # Reshape the input data to match the model's input shape\n",
    "        bow = bow.reshape(1, -1)  # Reshape to (1, 301)\n",
    "\n",
    "        # Predict using the model\n",
    "        results = model.predict(bow)[0]\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        # Print response based on the predicted tag\n",
    "        for intent in data[\"intents\"]:\n",
    "            if intent['tag'] == tag:\n",
    "                print(\"Bot:\", random.choice(intent['responses']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Talking with the bot(type quit to stop!)\n",
      "Bot: Wash the cut properly to prevent infection and stop the bleeding by applying pressure for 1-2minutes until bleeding stops. Apply Petroleum Jelly to make sure that the wound is moist for quick healing. Finally cover the cut with a sterile bandage. Pain relievers such as acetaminophen can be applied.\n",
      "Bot: Testicle pain should be assessed by a healthcare provider immediately, especially if the pain is severe or sudden. At home, you can ease discomfort by supporting the testicles with an athletic supporter and applying ice packs wrapped in a cloth to the area. Over-the-counter pain relievers like ibuprofen can help reduce pain and swelling. Avoid any activities that could worsen the pain. Seek emergency medical treatment if the pain is associated with nausea, fever, or abdominal pain.\n",
      "Bot: Testicle pain can be due to various causes including injuries, infections, or medical conditions like epididymitis. Rest and support the scrotum using a folded towel, apply ice packs to reduce swelling, and take over-the-counter pain relievers. Seek immediate medical attention if the pain is severe or sudden.\n",
      "Bot: Testicle pain should be assessed by a healthcare provider immediately, especially if the pain is severe or sudden. At home, you can ease discomfort by supporting the testicles with an athletic supporter and applying ice packs wrapped in a cloth to the area. Over-the-counter pain relievers like ibuprofen can help reduce pain and swelling. Avoid any activities that could worsen the pain. Seek emergency medical treatment if the pain is associated with nausea, fever, or abdominal pain.\n",
      "Bot: Testicle pain can be due to various causes including injuries, infections, or medical conditions like epididymitis. Rest and support the scrotum using a folded towel, apply ice packs to reduce swelling, and take over-the-counter pain relievers. Seek immediate medical attention if the pain is severe or sudden.\n",
      "Bot: Call emergency services right away. While waiting for help, apply pressure to the bleeding site with a clean cloth. If the bleeding does not stop, continue to apply pressure with a new cloth on top of the old. Keep the arm elevated above the heart to reduce bleeding.\n"
     ]
    }
   ],
   "source": [
    "chat() \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
